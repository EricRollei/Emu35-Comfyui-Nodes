# ComfyUI Emu 3.5 Nodes

This custom node allows you to run the Emu 3.5 model (Text-to-Image) in ComfyUI.

## Installation

1.  **Clone the Emu 3.5 Repository:**
    You must clone the official Emu 3.5 repository into a folder named `Emu3_5_repo` inside this node's directory.
    ```bash
    cd ComfyUI/custom_nodes/emu35
    git clone https://github.com/baaivision/Emu3.5 Emu3_5_repo
    ```

2.  **Install Requirements:**
    ```bash
    pip install -r requirements.txt
    ```
    Note: You might need to install `flash-attn` separately depending on your system.

3.  **Download Weights:**
    *   **Emu 3.5 Image Model:**
        Download the Emu 3.5 weights from Hugging Face: [BAAI/Emu3.5-Image](https://huggingface.co/BAAI/Emu3.5-Image)
        Place the files (including `config.json`, `*.safetensors`, `tokenizer_config.json`, etc.) in a folder, e.g., `ComfyUI/models/emu35/Emu3.5-Image`.
    
    *   **Vision Tokenizer:**
        Download the Vision Tokenizer weights from Hugging Face: [BAAI/Emu3.5-VisionTokenizer](https://huggingface.co/BAAI/Emu3.5-VisionTokenizer)
        Place the files (`model.ckpt` and `config.yaml`) in a folder, e.g., `ComfyUI/models/emu35/vision_tokenizer`.

## Usage

1.  **Emu 3.5 Loader:**
    - **model_name**: Select the model checkpoint (any file inside the `Emu3.5-Image` folder will work, the node finds the directory).
    - **vq_path**: Path to the vision tokenizer folder (e.g., `models/emu35/vision_tokenizer`). It supports pointing to the folder or the `model.ckpt` file directly.
    - **precision**: Choose `bf16` (recommended) or `nf4` for 4-bit quantization.

2.  **Emu 3.5 Sampler:**
    - Connect the model, tokenizer, and VQ model from the loader.
    - Enter your prompt.
    - Adjust width, height, steps, and CFG.

## Quantization

To create a 4-bit quantized model (nF4) for faster loading and lower VRAM usage:

1.  Run the `quantize.py` script:
    ```bash
    python quantize.py --model_path path/to/original/model --output_path path/to/save/quantized/model
    ```
2.  Then load the quantized model using the Loader node with `precision="nf4"`.
