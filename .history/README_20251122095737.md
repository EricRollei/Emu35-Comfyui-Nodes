# ComfyUI Emu 3.5 Nodes

This custom node allows you to run the Emu 3.5 model (Text-to-Image) in ComfyUI.

## Installation

1.  **Clone the Emu 3.5 Repository:**
    You must clone the official Emu 3.5 repository into a folder named `Emu3_5_repo` inside this node's directory.
    ```bash
    cd ComfyUI/custom_nodes/emu35
    git clone https://github.com/baaivision/Emu3.5 Emu3_5_repo
    ```

2.  **Install Requirements:**
    ```bash
    pip install -r requirements.txt
    ```
    Note: You might need to install `flash-attn` separately depending on your system.

3.  **Download Weights:**
    *   **Emu 3.5 Image Model:**
        Download the Emu 3.5 weights from Hugging Face: [BAAI/Emu3.5-Image](https://huggingface.co/BAAI/Emu3.5-Image)
        Place the files (including `config.json`, `*.safetensors`, `tokenizer_config.json`, etc.) in a folder named `Emu3.5-Image`.
    
    *   **Emu 3.5 Base Model (For VQA):**
        Download the Emu 3.5 Base weights from Hugging Face: [BAAI/Emu3.5](https://huggingface.co/BAAI/Emu3.5)
        Place the files in a folder named `Emu3.5-Base`.

    *   **Vision Tokenizer:**
        Download the Vision Tokenizer weights from Hugging Face: [BAAI/Emu3.5-VisionTokenizer](https://huggingface.co/BAAI/Emu3.5-VisionTokenizer)
        Place the files (`model.ckpt` and `config.yaml`) in a folder named `vision_tokenizer`.

    **Recommended Folder Structure:**
    ```
    ComfyUI/models/emu35/
    ├── Emu3.5-Image/       <-- Contains config.json, model-00001..., tokenizer_config.json
    ├── Emu3.5-Base/        <-- Contains config.json, model-00001..., tokenizer_config.json
    └── vision_tokenizer/   <-- Contains model.ckpt, config.yaml
    ```

## Usage

1.  **Emu 3.5 Loader:**
    - **model_name**: Select the model folder (e.g., `Emu3.5-Image` for generation, `Emu3.5-Base` for VQA).
    - **vq_model_name**: Select the vision tokenizer folder (e.g., `vision_tokenizer`).
    - **precision**: Choose `bf16` (recommended) or `nf4` for 4-bit quantization.

2.  **Emu 3.5 Sampler:**
    - Connect the model, tokenizer, and VQ model from the loader.
    - Enter your prompt.
    - Adjust width, height, steps, and CFG.

## Quantization

To create a 4-bit quantized model (nF4) for faster loading and lower VRAM usage:

1.  Run the `quantize.py` script:
    ```bash
    python quantize.py --model_path path/to/original/model --output_path path/to/save/quantized/model
    ```
2.  Then load the quantized model using the Loader node with `precision="nf4"`.
